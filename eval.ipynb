{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed for reproducability\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model, rate=None, custom_objects={}):\n",
    "    \"\"\"\n",
    "    Enables the droput layer - used for monte carlo droput based uncertainty computation\n",
    "    Note: the weights needs to be reloaded after calling this model\n",
    "    >>> model = enable_dropout(model)\n",
    "    >>> model.load_weights('path to model weight')\n",
    "    :param model:\n",
    "    :param rate:\n",
    "    :param custom_objects:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if(rate is not None): assert rate >= 0 and rate < 1, 'dropout rate is out of range'\n",
    "\n",
    "    model_config = model.get_config()\n",
    "    for i in range(len(model_config['layers'])):\n",
    "        class_name = model_config['layers'][i]['class_name']\n",
    "        if (class_name == 'SpatialDropout2D' or class_name =='Dropout' ):\n",
    "            model_config['layers'][i]['inbound_nodes'][0][0][-1]['training'] = True\n",
    "            if (rate is not None): model_config['layers'][i]['config']['rate'] = rate\n",
    "            #print('dropout enabled')\n",
    "\n",
    "    model = tf.keras.models.Model.from_config(model_config, custom_objects=custom_objects)\n",
    "    return model\n",
    "\n",
    "def load_model(file_name_prefix, model_directory, model, custom_objects=None, **kwarg):\n",
    "    \"\"\"\n",
    "    Loads keras model saved as hd5 and json defination\n",
    "    :param file_name_prefix:  prefix of the file name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    print(file_name_prefix)\n",
    "    print(model_directory)\n",
    "\n",
    "    json = os.path.join(os.getcwd(), model_directory, file_name_prefix + '.json')\n",
    "    with open(json) as j:\n",
    "        json_string = j.read()\n",
    "\n",
    "    if (model is None):\n",
    "        amodel = tf.keras.models.model_from_json(json_string, custom_objects=custom_objects)\n",
    "    else:\n",
    "        amodel = model\n",
    "\n",
    "    if ('enable_dropout' in kwarg and kwarg['enable_dropout'] == True):\n",
    "        rate = kwarg['dropout_rate'] if 'dropout_rate' in kwarg else None\n",
    "        print('Loading model by enabling dropout.')\n",
    "        amodel = enable_dropout(amodel, custom_objects=custom_objects, rate=rate)\n",
    "\n",
    "    amodel.load_weights(os.path.join(model_directory, file_name_prefix + '.hd5'))\n",
    "    return amodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent_clf\n",
      "savedmodel\n",
      "Loading model by enabling dropout.\n"
     ]
    }
   ],
   "source": [
    "#Load the MCD bayes model by enabling dropout in test phase\n",
    "model_prefix = 'intent_clf'\n",
    "model_save_dir = 'savedmodel'\n",
    "bayes_model_logit = load_model(model_prefix, model_save_dir, None,  enable_dropout=True, rate=0.2)\n",
    "# The saved model outputs the logits score. Therefore add the softmax layer to get probability of classes\n",
    "sm_layer = tf.keras.layers.Activation('softmax', name='smact') (bayes_model_logit.output) \n",
    "bayes_model = tf.keras.Model(bayes_model_logit.input, sm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mcd(features_in, T=50):\n",
    "    \"\"\"\n",
    "    Use monte carlo dropout to compute the prediction as well as uncertianty of predictions\n",
    "    params T: number of monte carlo iterations\n",
    "    \"\"\"\n",
    "  \n",
    "    N_class = bayes_model.outputs[0].get_shape().as_list()[1]\n",
    "    entropy_func = lambda x: -1 * np.sum(np.log(x + np.finfo(np.float32).eps) * x, axis=1)\n",
    "    predictive_prob_total = np.zeros((features_in.shape[0], N_class))\n",
    "    for i in range(T):\n",
    "        predictive_prob_total += bayes_model.predict(features_in)\n",
    "\n",
    "    predictive_prob_average = predictive_prob_total / (T * 1.0) \n",
    "    uncertainty = entropy_func(predictive_prob_average)\n",
    "    return predictive_prob_average, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load test inscope features\n",
    "\n",
    "model_name ='paraphrase-distilroberta-base-v1'\n",
    "npzfile = np.load('train_val_test_'+model_name+'.npz',allow_pickle=True)\n",
    "\n",
    "val_features = npzfile['val_features']\n",
    "val_labels = npzfile['val_labels']\n",
    "test_features = npzfile['test_features']\n",
    "test_labels = npzfile['test_labels']\n",
    "label_map, label_map_inv = npzfile['label_maps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inscore accuracy 0.9215555555555556\n"
     ]
    }
   ],
   "source": [
    "# Predict on inscope test set and compute accuracy \n",
    "test_pred, uncertainty = predict_mcd(test_features)\n",
    "insc_acc = accuracy_score(test_labels, test_pred.argmax(axis=1))\n",
    "print('Inscore accuracy', insc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.9497888315344909  on confident samples 4262 out of  4500 samples\n"
     ]
    }
   ],
   "source": [
    "#Check in-score accuracy on the samples with lower uncertainty i.e., confident samples \n",
    "idx_good= np.where(uncertainty<1.3)\n",
    "acc_conf = accuracy_score(test_labels[idx_good], test_pred.argmax(axis=1)[idx_good])\n",
    "print('Accuracy=',acc_conf, ' on confident samples', len(idx_good[0]), 'out of ', len(uncertainty), 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use uncertainty of classify oos test set\n",
    "npzfile_oos = np.load('train_val_test_oos_'+model_name+'.npz',allow_pickle=True)\n",
    "train_features_oos = npzfile_oos['train_features_oos']\n",
    "train_labels_oos = 1-npzfile_oos['train_labels_oos'] # set all zeros to 1\n",
    "val_features_oos = npzfile_oos['val_features_oos']\n",
    "val_labels_oos = 1-npzfile_oos['val_labels_oos'] # set all zeros to 1\n",
    "test_features_oos = npzfile_oos['test_features_oos']\n",
    "test_labels_oos =  1-npzfile_oos['test_labels_oos'] # set all zeros to 1\n",
    "label_map_oos, label_map_inv_oos = npzfile_oos['label_maps_oos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a single oos trainval set for parameter estimation by adding fraction of inscope val set(by setting 0 labels)\n",
    "Nval_tune=1500\n",
    "idx_temp = np.random.permutation(val_features.shape[0])[:Nval_tune]\n",
    "trainval_features_oos = np.vstack([train_features_oos, val_features_oos, val_features[idx_temp] ])\n",
    "trainval_labels_oos = np.concatenate([train_labels_oos, val_labels_oos, np.zeros(Nval_tune,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainval_pred_intent_scores_oos  denotes the score for a feature to be classified as one of the intents. \n",
    "#When the input feature does not belong to the distribution of the train set(in scope), we expect higher uncertainty associated with it.\n",
    "#Therefore the the uncertainty alone  can be used to detect oos case.\n",
    "trainval_pred_intent_scores_oos, trainval_uncertainty = predict_mcd(trainval_features_oos)\n",
    "#Search for an optimal threshold using f1score as criterion\n",
    "grid = np.arange(0,trainval_uncertainty.max(),0.1)\n",
    "fscores = []\n",
    "for i in grid:\n",
    "    predi = (trainval_uncertainty > i ).astype(np.uint8)\n",
    "    fscore = f1_score(trainval_labels_oos, predi)\n",
    "    fscores.append(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty threshold that maximizes f1 score  1.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'f1score')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_threshold = grid[np.argmax(fscores)]\n",
    "print('Uncertainty threshold that maximizes f1 score ', optimal_threshold)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(grid, fscores)\n",
    "plt.xlabel('uncertainty')\n",
    "plt.ylabel('f1score')\n",
    "#plt,show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How this method works in deployment phase?\n",
    "#When uncertainty <=optimal_threshold - use test_pred_intent_scores_oos to choose one of the intent\n",
    "#When uncertaity > optimal_threshold - the feature belongs to oos class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall =  0.653 Precision = 0.7403628117913832 Fscore= 0.69394261424017\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on a combined set of  test_features_oos (labels 1) and test_features (label 0)\n",
    "test_features_ins_oos = np.vstack([test_features, test_features_oos])\n",
    "test_labels_ins_oos = np.concatenate([ np.zeros(test_features.shape[0]),np.ones(test_features_oos.shape[0])])\n",
    "test_pred_intent_scores_ins_oos, test_uncertainty = predict_mcd(test_features_ins_oos)\n",
    "test_pred_ins_oos = (test_uncertainty > optimal_threshold).astype(np.uint8)\n",
    "\n",
    "#Compute evaluation metrics\n",
    "recall = recall_score(test_labels_ins_oos, test_pred_ins_oos)\n",
    "precision  = precision_score(test_labels_ins_oos, test_pred_ins_oos)\n",
    "f1score = f1_score(test_labels_ins_oos, test_pred_ins_oos)\n",
    "print('Recall = ', recall, 'Precision =', precision, 'Fscore=', f1score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
